{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Downlod Videos'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Downlod Videos'''\n",
    "\n",
    "\n",
    "# from pytube import YouTube\n",
    "# yt = YouTube('https://youtu.be/jdWZjGLxcaY')\n",
    "# print(yt.title)\n",
    "# print(yt.thumbnail_url)\n",
    "# stream = yt.streams.filter(progressive=True, file_extension='mp4').first()\n",
    "# stream.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Creating Mapping.csv file'''\n",
    "\n",
    "\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "\n",
    "# mypath1 = '/Users/mac16_anyang/Documents/Data Science/video_processing'\n",
    "# onlyfiles = [h for h in listdir(mypath1) if isfile(join(mypath1, h))]\n",
    "\n",
    "# from os import walk\n",
    "\n",
    "# h = []\n",
    "# for (dirpath, dirnames, filenames) in walk(mypath1):\n",
    "#     h.extend(filenames)\n",
    "#     break\n",
    "    \n",
    "\n",
    "# mypath2 = '/Users/mac16_anyang/Documents/Data Science/video_processing/tom'\n",
    "# onlyfiles = [t for t in listdir(mypath2) if isfile(join(mypath2, t))]\n",
    "\n",
    "# t = []\n",
    "# for (dirpath, dirnames, filenames) in walk(mypath2):\n",
    "#     t.extend(filenames)\n",
    "#     break    \n",
    "    \n",
    "    \n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(h)\n",
    "# df['Class'] = 0\n",
    "# df.rename(columns={0:'Image_ID'},inplace = True)\n",
    "\n",
    "\n",
    "# df1 = pd.DataFrame(t)\n",
    "# df1['Class'] = 1\n",
    "# df1.rename(columns={0:'Image_ID'},inplace = True)\n",
    "\n",
    "\n",
    "# data = pd.concat([df,df1],axis=0)\n",
    "# data = data.set_index('Image_ID')\n",
    "# data.to_csv('mapping.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python\n",
    "\n",
    "import cv2     # for capturing videos\n",
    "import math   # for mathematical operations\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "from keras.utils import np_utils     #  utility package for keras\n",
    "from skimage.transform import resize   # for resizing images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step – 1: Read the video, extract frames from it and save them as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Capture Images from Video'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Capture Images from Video'''\n",
    "\n",
    "\n",
    "# videoFile = \"tom1.mp4\"\n",
    "# cap = cv2.VideoCapture(videoFile)   # capturing the video from the given path\n",
    "# frameRate = 50 #frame rate cap.get(5)\n",
    "# x = 660\n",
    "# while(cap.isOpened()):\n",
    "#     frameId = cap.get(1) #current frame number\n",
    "#     ret, frame = cap.read()\n",
    "#     if (ret != True):\n",
    "#         break\n",
    "#     if (frameId % math.floor(frameRate) == 0):\n",
    "#         filename =\"frame%d.jpg\" % x;x+=1\n",
    "#         cv2.imwrite(filename, frame)\n",
    "# cap.release()\n",
    "# print (\"Done!\")\n",
    "\n",
    "# img = plt.imread('frame0.jpg')   # reading image using its name\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step – 2: Label a few images for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('mapping.csv')     # reading the csv file\n",
    "# data = data[data.Image_ID != '.DS_Store']\n",
    "# data.head()      # printing first five rows of the file\n",
    "\n",
    "# X = [ ]     # creating an empty array\n",
    "# for img_name in data.Image_ID:\n",
    "#     img = plt.imread(img_name)\n",
    "#     X.append(img)  # storing each image in array X\n",
    "# X = np.array(X)    # converting list to array\n",
    "\n",
    "\n",
    "# y = data.Class\n",
    "# dummy_y = np_utils.to_categorical(y)    # one hot encoding Classes\n",
    "\n",
    "\n",
    "# image = []\n",
    "# for i in range(0,X.shape[0]):\n",
    "#     a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\n",
    "#     image.append(a)\n",
    "# X = np.array(image)\n",
    "\n",
    "\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# X = preprocess_input(X)      # preprocessing the input data\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)    # preparing the validation set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 25,693,186\n",
      "Trainable params: 25,693,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.1737 - accuracy: 0.9209 - val_loss: 0.0196 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 0.0102 - val_accuracy: 0.9947\n",
      "Epoch 4/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 0.9947\n",
      "Epoch 5/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9947\n",
      "Epoch 6/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 0.9947\n",
      "Epoch 7/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 0.9947\n",
      "Epoch 8/30\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 0.9947\n",
      "Epoch 9/30\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 0.9947\n",
      "Epoch 10/30\n",
      "28/28 [==============================] - 2s 71ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9947\n",
      "Epoch 11/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 0.9947\n",
      "Epoch 12/30\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 0.9947\n",
      "Epoch 13/30\n",
      "28/28 [==============================] - 2s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 0.9947\n",
      "Epoch 14/30\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 9.3546e-04 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 0.9947\n",
      "Epoch 15/30\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 7.9847e-04 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 0.9947\n",
      "Epoch 16/30\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 7.5091e-04 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 0.9947\n",
      "Epoch 17/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 8.2627e-04 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 0.9947\n",
      "Epoch 18/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 6.6452e-04 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 0.9947\n",
      "Epoch 19/30\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 5.2397e-04 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 0.9947\n",
      "Epoch 20/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 5.7698e-04 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 0.9947\n",
      "Epoch 21/30\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 4.0033e-04 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 0.9947\n",
      "Epoch 22/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 4.3431e-04 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 0.9947\n",
      "Epoch 23/30\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 4.5580e-04 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9947\n",
      "Epoch 24/30\n",
      "28/28 [==============================] - 2s 77ms/step - loss: 3.2082e-04 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9947\n",
      "Epoch 25/30\n",
      "28/28 [==============================] - 2s 75ms/step - loss: 3.3069e-04 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9947\n",
      "Epoch 26/30\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 3.1238e-04 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9947\n",
      "Epoch 27/30\n",
      "28/28 [==============================] - 2s 72ms/step - loss: 3.7668e-04 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 0.9947\n",
      "Epoch 28/30\n",
      "28/28 [==============================] - 2s 76ms/step - loss: 3.0983e-04 - accuracy: 1.0000 - val_loss: 0.0231 - val_accuracy: 0.9947\n",
      "Epoch 29/30\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 2.6454e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9947\n",
      "Epoch 30/30\n",
      "28/28 [==============================] - 2s 73ms/step - loss: 2.8657e-04 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 0.9947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd5e57ea2d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout\n",
    "\n",
    "# i.Make predictions using this model, and then use those features to retrain the model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer\n",
    "\n",
    "\n",
    "X_train = base_model.predict(X_train)\n",
    "X_valid = base_model.predict(X_valid)\n",
    "X_train.shape, X_valid.shape\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(885, 7*7*512)      # converting to 1-D\n",
    "X_valid = X_valid.reshape(380, 7*7*512)\n",
    "\n",
    "\n",
    "\n",
    "train = X_train/X_train.max()      # centering the data\n",
    "X_valid = X_valid/X_train.max()\n",
    "\n",
    "\n",
    "# ii. Building the model\n",
    "model = Sequential()\n",
    "model.add(InputLayer((7*7*512,)))    # input layer\n",
    "model.add(Dropout(0.2)) # dropout layer\n",
    "model.add(Dense(units=1024, activation='sigmoid')) # hidden layer\n",
    "model.add(Dropout(0.2)) # dropout layer\n",
    "model.add(Dense(2, activation='softmax'))    # output layer\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# iii. Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# iv. Training the model\n",
    "model.fit(train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# predictions = model.predict_classes(test_image)\n",
    "\n",
    "# scores = model.evaluate(test_image, test_y)\n",
    "# print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# his = pd.DataFrame(model.history.history)\n",
    "# his.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath1 = '/Users/mac16_anyang/Documents/Data Science/video_processing'\n",
    "# onlyfiles = [h for h in listdir(mypath1) if isfile(join(mypath1, h))]\n",
    "\n",
    "from os import walk\n",
    "\n",
    "h = []\n",
    "file = []\n",
    "for (dirpath, dirnames, filenames) in walk(mypath1):\n",
    "    h.extend(filenames)\n",
    "    break\n",
    "    \n",
    "for name in h:\n",
    "        if name.split('.')[-1] == 'jpg':\n",
    "            if name.split('.')[0] == 'cat':\n",
    "                file.append([name,0])\n",
    "            elif name.split('.')[0] == 'dog':\n",
    "                file.append([name,1])\n",
    "\n",
    "df = pd.DataFrame(file)\n",
    "df.columns = ['name','label']\n",
    "\n",
    "X = [ ]     # creating an empty array\n",
    "for img_name in df.name:\n",
    "    img = plt.imread(img_name)\n",
    "    X.append(img)  # storing each image in array X\n",
    "X = np.array(X)    # converting list to array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df.label\n",
    "dummy_y = np_utils.to_categorical(y) \n",
    "\n",
    "image = []\n",
    "for i in range(0,X.shape[0]):\n",
    "    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\n",
    "    image.append(a)\n",
    "X = np.array(image)\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "X = preprocess_input(X)      # preprocessing the input data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 25,693,186\n",
      "Trainable params: 25,693,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.1255 - accuracy: 0.9495 - val_loss: 0.0736 - val_accuracy: 0.9696\n",
      "Epoch 2/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.0353 - accuracy: 0.9898 - val_loss: 0.0660 - val_accuracy: 0.9754\n",
      "Epoch 3/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.0147 - accuracy: 0.9971 - val_loss: 0.0709 - val_accuracy: 0.9717\n",
      "Epoch 4/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.0068 - accuracy: 0.9996 - val_loss: 0.0724 - val_accuracy: 0.9746\n",
      "Epoch 5/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 0.0723 - val_accuracy: 0.9754\n",
      "Epoch 6/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9758\n",
      "Epoch 7/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9746\n",
      "Epoch 8/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0771 - val_accuracy: 0.9758\n",
      "Epoch 9/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 8.7268e-04 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9767\n",
      "Epoch 10/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 6.8806e-04 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9767\n",
      "Epoch 11/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 6.1768e-04 - accuracy: 1.0000 - val_loss: 0.0813 - val_accuracy: 0.9754\n",
      "Epoch 12/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 4.9531e-04 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9767\n",
      "Epoch 13/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 3.5202e-04 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9758\n",
      "Epoch 14/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 2.7548e-04 - accuracy: 1.0000 - val_loss: 0.0874 - val_accuracy: 0.9771\n",
      "Epoch 15/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 2.6404e-04 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9750\n",
      "Epoch 16/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 2.1260e-04 - accuracy: 1.0000 - val_loss: 0.0886 - val_accuracy: 0.9767\n",
      "Epoch 17/30\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 1.6398e-04 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9750\n",
      "Epoch 18/30\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 1.7968e-04 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9758\n",
      "Epoch 19/30\n",
      "175/175 [==============================] - 14s 77ms/step - loss: 1.4355e-04 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9771\n",
      "Epoch 20/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 1.1993e-04 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9767\n",
      "Epoch 21/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 9.4043e-05 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9758\n",
      "Epoch 22/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 9.1146e-05 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 0.9758\n",
      "Epoch 23/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 7.2401e-05 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 0.9762\n",
      "Epoch 24/30\n",
      "175/175 [==============================] - 14s 77ms/step - loss: 7.2128e-05 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9758\n",
      "Epoch 25/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 6.0781e-05 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9767\n",
      "Epoch 26/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 7.7745e-05 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9762\n",
      "Epoch 27/30\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 5.8761e-05 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 0.9758\n",
      "Epoch 28/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 4.8600e-05 - accuracy: 1.0000 - val_loss: 0.1008 - val_accuracy: 0.9762\n",
      "Epoch 29/30\n",
      "175/175 [==============================] - 14s 77ms/step - loss: 4.2045e-05 - accuracy: 1.0000 - val_loss: 0.1011 - val_accuracy: 0.9754\n",
      "Epoch 30/30\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 4.0523e-05 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe55feca050>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.layers import Dense, InputLayer, Dropout\n",
    "\n",
    "# # i.Make predictions using this model, and then use those features to retrain the model\n",
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer\n",
    "\n",
    "\n",
    "# X_train = base_model.predict(X_train)\n",
    "# X_valid = base_model.predict(X_valid)\n",
    "# X_train.shape, X_valid.shape\n",
    "\n",
    "\n",
    "# X_train = X_train.reshape(len(X_train), 7*7*512)      # converting to 1-D\n",
    "# X_valid = X_valid.reshape(len(X_valid), 7*7*512)\n",
    "\n",
    "\n",
    "\n",
    "# train = X_train/X_train.max()      # centering the data\n",
    "# X_valid = X_valid/X_train.max()\n",
    "\n",
    "\n",
    "# ii. Building the model\n",
    "model = Sequential()\n",
    "model.add(InputLayer((7*7*512,)))    # input layer\n",
    "model.add(Dropout(0.2)) # dropout layer\n",
    "model.add(Dense(units=1024, activation='sigmoid')) # hidden layer\n",
    "model.add(Dropout(0.2)) # dropout layer\n",
    "model.add(Dense(2, activation='softmax'))    # output layer\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# iii. Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# iv. Training the model\n",
    "model.fit(train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath1 = '/Users/mac16_anyang/Documents/Data Science/video_processing'\n",
    "# onlyfiles = [h for h in listdir(mypath1) if isfile(join(mypath1, h))]\n",
    "\n",
    "from os import walk\n",
    "\n",
    "h2 = []\n",
    "file2 = []\n",
    "for (dirpath, dirnames, filenames) in walk(mypath1):\n",
    "    h2.extend(filenames)\n",
    "    break\n",
    "    \n",
    "for name in h2:\n",
    "        if name.split('.')[-1] == 'jpg':\n",
    "            if name.split('.')[0] == 'cat':\n",
    "                file2.append([name,0])\n",
    "            elif name.split('.')[0] == 'dog':\n",
    "                file2.append([name,1])\n",
    "\n",
    "df2 = pd.DataFrame(file2)\n",
    "df2.columns = ['name','label']\n",
    "\n",
    "X = [ ]     # creating an empty array\n",
    "for img_name in df2.name:\n",
    "    img = plt.imread(img_name)\n",
    "    X.append(img)  # storing each image in array X\n",
    "X = np.array(X)    # converting list to array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2.label\n",
    "dummy_y = np_utils.to_categorical(y) \n",
    "\n",
    "image = []\n",
    "for i in range(0,X.shape[0]):\n",
    "    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\n",
    "    image.append(a)\n",
    "X = np.array(image)\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "X = preprocess_input(X)      # preprocessing the input data\n",
    "X_test = base_model.predict(X)\n",
    "\n",
    "\n",
    "X_test = X_test.reshape(len(X_test), 7*7*512)      # converting to 1-D\n",
    "test = X_test/X_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3905 - accuracy: 0.9459\n",
      "accuracy: 94.59%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict_classes(test)\n",
    "\n",
    "scores = model.evaluate(test, dummy_y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
